<!-- start header -->
To run this example locally, execute: `ploomber examples -n spec-api-python`.

To start a free, hosted JupyterLab: [![binder-logo](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ploomber/binder-env/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Fploomber%252Fprojects%26urlpath%3Dlab%252Ftree%252Fprojects%252Fspec-api-python%252FREADME.ipynb%26branch%3Dmaster)

Found an issue? [Let us know.](https://github.com/ploomber/projects/issues/new?title=spec-api-python%20issue)

Have questions? [Ask us anything on Slack.](http://community.ploomber.io/)

For a notebook version (with outputs) of this file, [click here](https://github.com/ploomber/projects/blob/master/spec-api-python/README.ipynb)
<!-- end header -->




# Your first Python pipeline

This guide shows you how to run your first Ploomber pipeline.

**Note:** This is intended for a quick and interactive experience. If you want
to learn about Ploomber's core concepts and design rationale, go to the
[the next tutorial](https://ploomber.readthedocs.io/en/stable/get-started/basic-concepts.html)


## Description

This pipeline contains 3 **tasks**, the first task `get.py` gets some data,
`clean.py` cleans it and `plot.py` generates a visualization:

```bash
ls *.py
```

These three scripts make up our pipeline (or **DAG**), which is a collection
of tasks with a pre-defined execution order.

**Note:** These tasks are Python scripts, but you can use functions, notebooks,
and even SQL scripts. The next guide explains how other types of tasks work.

Ploomber integrates with Jupyter. If you open the scripts inside the
`jupyter notebook` app, they will render as notebooks. If you're using `jupyter lab`, you need to right click -> open with -> Notebook as depicted below:

![lab-open-with-nb](https://ploomber.io/doc/lab-open-with-notebook.png)

Along with the `*.py` files, there is a `pipeline.yaml` file where we declare
which files we use as tasks:

<!-- #md -->
```yaml
# Content of pipeline.yaml
tasks:
  - source: raw.py
    product:
      nb: output/raw.ipynb
      data: output/data.csv

  - source: clean.py
    product:
      nb: output/clean.ipynb
      data: output/clean.csv

  - source: plot.py
    product: output/plot.ipynb

```
<!-- #endmd -->

**Note:** The `pipeline.yaml` file is optional, but it gives you more flexibility.
[Click here](https://github.com/ploomber/projects/tree/master/spec-api-directory) to see an example without a `pipeline.yaml` file.


Let's plot the pipeline:

```bash
ploomber plot
```

```python
from IPython.display import Image
Image(filename='pipeline.png')
```

The `status` command gives us an overview of the pipeline:

```bash
ploomber status
```

## How is execution order determined?

Ploomber infers the pipeline structure from your code. If task B uses the output from
task A as input, we say A is an **upstream** dependency of B. For example, to
clean the data, we must get it first; hence, we declare the following in `clean.py`:

~~~python
# execute 'raw" task before 'clean'
upstream = ['raw']
~~~

Once we finish cleaning the data, we must save it somewhere (this is known
as a **product**). Products can be files or SQL relations. Our current example
only generates files.

To specify where to save the output of each task, we use the `product`
key. For example, the `raw` task definition looks like this:

~~~yaml
- source: raw.py
  product:
    nb: output/raw.ipynb
    data: output/data.csv
~~~


Scripts and notebooks automatically generate a copy of themselves in Jupyter
notebook format (`.ipynb`). That's why we see a notebook in the `product`
dictionary (`nb` key). The notebook format allows us to generate standalone
files with charts and tables, no need to write extra code to save our charts!

Notebooks as pipeline products are crucial concepts: `raw.py` is part of the pipeline's
source code but `output/raw.ipynb` is not. It is an artifact generated by the source code.

If you don't want to generate output notebooks, you can use a Python function
as tasks. Our upcoming tutorial goes deeper into the different types of tasks available.

## Building the pipeline

Let's build the pipeline:

```bash
mkdir output
ploomber build
```

This pipeline saves all the output in the `output/` directory; we have a few
data files:

```bash
ls output/*.csv
```

And a notebook for each script:

```bash
ls output/*.ipynb
```

## Updating the pipeline

Quick experimentation is essential to develop a data pipeline. Ploomber allows
you to quickly run new experiments without having to keep track of tasks
dependencies.

Let's say you found a problematic column in the data and want to add more
cleaning logic to your `clean.py` script. `raw.py` does not depend
on `clean.py`, but `plot.py` does. If you modify `clean.py`, you'd have
to execute `clean.py` and then `plot.py` to bring your pipeline up-to-date.

As your pipeline grows, keeping track of task dependencies gets time-consuming.
Ploomber does that for you and only executes outdated tasks on each run.

Make some changes to the `clean.py` script, then build again:

```bash
ploomber build
```

You'll see that `raw.py` didn't run because it was not affected by the change! Try modifying any of the other tasks, then come
back and run `ploomber build`.


## Where to go from here

This tutorial showed how to build a pipeline with Ploomber; however, it only
superficially covered Ploomber's core concepts and design rationale. [the upcoming
tutorial](https://ploomber.readthedocs.io/en/stable/get-started/basic-concepts.html)
goes deeper in those terms.