{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6df005",
   "metadata": {},
   "source": [
    "<!-- start header -->\n",
    "To run this locally, [install Ploomber](https://docs.ploomber.io/en/latest/get-started/quick-start.html) and execute: `ploomber examples -n templates/mlflow`\n",
    "\n",
    "[![binder-logo](https://raw.githubusercontent.com/ploomber/projects/master/_static/open-in-jupyterlab.svg)](https://binder.ploomber.io/v2/gh/ploomber/binder-env/main?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Fploomber%252Fprojects%26urlpath%3Dlab%252Ftree%252Fprojects%252Ftemplates/mlflow%252FREADME.ipynb%26branch%3Dmaster)\n",
    "\n",
    "Questions? [Ask us on Slack.](https://ploomber.io/community/)\n",
    "\n",
    "For a notebook version (with outputs) of this file, [click here](https://github.com/ploomber/projects/blob/master/templates/mlflow/README.ipynb)\n",
    "<!-- end header -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16613cb3",
   "metadata": {},
   "source": [
    "# Ploomber + MLflow\n",
    "\n",
    "*Note: This is an advanced tutorial and assumes familiarity with basic Ploomber concepts, you may want to read the basic tutorial first and then come back here.*\n",
    "\n",
    "*Note: This example requires ploomber 0.13.2 or higher.*\n",
    "\n",
    "<!-- start description -->\n",
    "Train a grid of models and log them to MLflow.\n",
    "<!-- end description -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let's first load our `pipeline.yaml` as a DAG object and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ploomber.spec import DAGSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9173b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAGSpec('pipeline.yaml').to_dag()\n",
    "\n",
    "# this is the same as doing \"ploomber plot\" from the command line\n",
    "dag.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424a9be",
   "metadata": {},
   "source": [
    "The pipeline gets the iris dataset (`get` task), creates a few features (`sepal`, and `petal` tasks), and joins everything into a single file (`features` task). Then we see eight tasks in parallel (`fit-0` to `fit-7`); they all use the same script as a source but train different models; we log the models to MLflow here. Finally, we have a task (`compare`) that uses MLflow's API to query the runs and prints the best overall experiment.\n",
    "\n",
    "Now that we have a high-level idea of what the pipeline is doing, let's look at the `pipeline.yaml`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9c976",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```yaml\n",
    "# Content of pipeline.yaml\n",
    "# uncomment to run in parallel\n",
    "# executor: parallel\n",
    "\n",
    "tasks:\n",
    "  - source: tasks.raw.get\n",
    "    product: products/raw/get.csv\n",
    "\n",
    "  - source: tasks.features.sepal\n",
    "    product: products/features/sepal.csv\n",
    "\n",
    "  - source: tasks.features.petal\n",
    "    product: products/features/petal.csv\n",
    "\n",
    "  - source: tasks.features.features\n",
    "    product: products/features/features.csv\n",
    "\n",
    "  - source: scripts/fit.py\n",
    "    name: fit-\n",
    "    # we need to turn this off because parameters depend on the type of model\n",
    "    static_analysis: disable\n",
    "    product: products/report.ipynb\n",
    "    grid:\n",
    "      - model: sklearn.ensemble.RandomForestClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        criterion: [gini, entropy]\n",
    "        params_names: [[n_estimators, criterion]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "      - model: sklearn.ensemble.AdaBoostClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        learning_rate: [0.5, 1.0]\n",
    "        params_names: [[n_estimators, learning_rate]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "    on_finish: hooks.store_report\n",
    "\n",
    "\n",
    "  - source: scripts/compare.py\n",
    "    product: products/compare.ipynb\n",
    "    params:\n",
    "      mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db3e31",
   "metadata": {},
   "source": [
    "The first four tasks are simple, they execute Python functions to prepare the data. The 5th and 6th entries are the interesting ones. Let's analyze the 5th entry first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66b50d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```yaml\n",
    "# Content of pipeline.yaml\n",
    "\n",
    "  - source: scripts/fit.py\n",
    "    name: fit-\n",
    "    # we need to turn this off because parameters depend on the type of model\n",
    "    static_analysis: disable\n",
    "    product: products/report.ipynb\n",
    "    grid:\n",
    "      - model: sklearn.ensemble.RandomForestClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        criterion: [gini, entropy]\n",
    "        params_names: [[n_estimators, criterion]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "      - model: sklearn.ensemble.AdaBoostClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        learning_rate: [0.5, 1.0]\n",
    "        params_names: [[n_estimators, learning_rate]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "    on_finish: hooks.store_report\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48dc98",
   "metadata": {},
   "source": [
    "The task uses `scripts/fit.py` as its source and sets the name to `fit-` so that generated tasks have the same prefix. Then we set `static_analysis` to `false`, the static analysis feature checks that the parameters passed to the task match the ones declared by the script (in the `parameters` cell). Still, since we'll be passing different parameters depending on the model, we have to turn off this feature. Finally, we can see that we'll store the output in `products/report.ipynb`.\n",
    "\n",
    "`grid` is where the magic happens. This feature allows us to pass a grid of parameters and generate many tasks from a single declaration. For example, the first element states we want to train a random forest and vary some hyperparameters (`n_estimator`, and `criterion`).\n",
    "\n",
    "`params_names` is a list of parameters that we are going to vary across experiments; we use this list to pass the values to the model constructor (since depend on the model type), for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# init parameters depend on model type\n",
    "model = RandomForestClassifier(n_estimators=10, criterion='gini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d2716",
   "metadata": {},
   "source": [
    "`track` controls whether to track experiments with MLflow or not; it's turned off by default because we may want to open `scripts/fit.py` and edit it interactively using Jupyter without tracking anything. `mlflow_tracking_uri` is the URI that we'll use for MLflow. Note that `track` and `mlflow_tracking_uri` are placeholders, and their values come from an `env.yaml` file, whose contents are shown next:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e1a26",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```yaml\n",
    "# Content of env.yaml\n",
    "track: false\n",
    "mlflow_tracking_uri: 'file:{{root}}/mlruns'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91618ec",
   "metadata": {},
   "source": [
    "We can see that `track` is set to `false`, and `mlflow_tracking_uri` to `file:{{root}}/mlruns`. Ploomber automatically resolves the `{{root}}` placeholder to the parent directory of our `pipeline.yaml` file, but we could have an explicit value instead (e.g., `file:/path/to/directory`).\n",
    "\n",
    "Let's take a look at the portion of `scripts/fit.py` that uses `track` and `mlflow_tracking_uri`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75abba",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```python\n",
    "# Content of scripts/fit.py\n",
    "\n",
    "# %%\n",
    "if track:\n",
    "    print('tracking with mlflow...')\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "    @atexit.register\n",
    "    def end_run():\n",
    "        mlflow.end_run()\n",
    "else:\n",
    "    print('tracking skipped...')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187446e",
   "metadata": {},
   "source": [
    "If `track` is `True`, we set the tracking URI and use the built-in `atexit` module to ensure that we call `mlflow.end_run` when our script finishes execution. If `track` is `False`, we mock `mlflow` (using a built-in module) so all our calls to `mlflow` don't do anything.\n",
    "\n",
    "\n",
    "Now that we explained what each parameter is doing, let's go back to our task declaration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87918543",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```yaml\n",
    "# Content of pipeline.yaml\n",
    "\n",
    "  - source: scripts/fit.py\n",
    "    name: fit-\n",
    "    # we need to turn this off because parameters depend on the type of model\n",
    "    static_analysis: disable\n",
    "    product: products/report.ipynb\n",
    "    grid:\n",
    "      - model: sklearn.ensemble.RandomForestClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        criterion: [gini, entropy]\n",
    "        params_names: [[n_estimators, criterion]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "      - model: sklearn.ensemble.AdaBoostClassifier\n",
    "        n_estimators: [10, 20]\n",
    "        learning_rate: [0.5, 1.0]\n",
    "        params_names: [[n_estimators, learning_rate]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "    on_finish: hooks.store_report\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f3660",
   "metadata": {},
   "source": [
    "The total number of tasks generated by grid depends on the number of parameters; for the first entry, we have:\n",
    "\n",
    "1. `model` (1 element)\n",
    "2. `n_estimators` (2)\n",
    "3. `criterion` (2)\n",
    "4. `params_names` (1) - see note below\n",
    "5. `track` (1)\n",
    "6. `mlflow_tracking_uri` (1)\n",
    "\n",
    "This will generate a total of `1 * 2 * 2 * 1 * 1 * 1 = 4` tasks.\n",
    "\n",
    "Note that `params_names` is a list with one element because by default, `grid` interprets lists as `\"create one task per element on the list\"`, but we don't want that. In this case, we want all tasks to receive the complete list, and we ensure this by passing a list with a single element.\n",
    "\n",
    "The next entry on the grid is similar and generates another four tasks.\n",
    "\n",
    "To see more clearly what parameters each task receives, let's use the `dag` object to print the parameters of the first two `fit-` tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784abfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "\n",
    "fm = black.FileMode()\n",
    "\n",
    "tasks_fit = [t for t in dag.values() if 'fit' in t.name]\n",
    "\n",
    "for task in tasks_fit[:2]:\n",
    "    if 'fit' in task.name:\n",
    "        params = black.format_str(str(task.params), mode=fm)\n",
    "        print(f'{task.name} params:\\n{params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975772a",
   "metadata": {},
   "source": [
    "You can see that most parameters are the same, except for `criterion` (the first one has `gini`, and the second one `entropy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af88b45",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "\n",
    "Every parameter declared in `env.yaml` can be switched from the command line; let's get the `--help` information to see how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ploomber build --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb36887",
   "metadata": {},
   "source": [
    "As you can see, we can use `--env--track` to switch the track flag. So let's run the pipeline and switch this value to ensure MLflow tracks our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# note: this will take about a minute to run\n",
    "ploomber build --env--track true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2349f5",
   "metadata": {},
   "source": [
    "That's it! We just trained eight models and logged them to MLflow! Here's how my MLflow looks like:\n",
    "\n",
    "![list](images/list.png)\n",
    "\n",
    "Note that `script/fit.py` organizes each model in a different MLflow experiment:\n",
    "\n",
    "![experiments](images/experiments.png)\n",
    "\n",
    "And logs all model parameters and metrics, but it also stores a confusion matrix and the model file:\n",
    "\n",
    "![artifacts](images/artifacts.png)\n",
    "\n",
    "But more importantly, it stores the executed notebook as an HTML file!\n",
    "\n",
    "![report](images/report.png)\n",
    "\n",
    "Storing the notebook helps debug: we can use `print` statements in our script to log any helpful information; furthermore, since this is an HTML file, it may contain other details such as printed data frames or charts. I'll explain how we achieve this in the next section.\n",
    "\n",
    "## Storing executed notebooks in MLflow\n",
    "\n",
    "Ploomber has a feature called \"hooks\", which allows running code when a task finishes execution. In our case, we're using it for converting the generated notebook to HTML and log it to MLflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cb698",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```python\n",
    "# Content of hooks.py\n",
    "import mlflow\n",
    "from nbconvert import HTMLExporter\n",
    "from sklearn_evaluation import NotebookIntrospector\n",
    "\n",
    "\n",
    "def store_report(product, params):\n",
    "    if params['track']:\n",
    "        nb = NotebookIntrospector(product)\n",
    "        run_id = nb['mlflow-run-id'].strip()\n",
    "\n",
    "        # https://nbconvert.readthedocs.io/en/latest/config_options.html#preprocessor-options\n",
    "        exporter = HTMLExporter()\n",
    "        # hide code cells\n",
    "        exporter.exclude_input = True\n",
    "        body, _ = exporter.from_filename(product)\n",
    "\n",
    "        with mlflow.start_run(run_id):\n",
    "            mlflow.log_text(body, 'nb.html')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31edcfb7",
   "metadata": {},
   "source": [
    "We add `product` and `params` to the function signature to tell Ploomber that we want the `product` that the task generates and the task params. We use this to load the generated notebook and convert it to HTML. Then we retrieve the run id and log it to MLflow.\n",
    "\n",
    "*Note: we're using sklearn_evaluation.NotebookIntrospector to retrieve mlflow's run id from the executed notebook, [click here](https://sklearn-evaluation.readthedocs.io/en/stable/user_guide/NotebookCollection.html) to learn more about how this works.*\n",
    "\n",
    "To register the hook, we add the `on_finish` entry to the task declaration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d461d28",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```yaml\n",
    "# Content of pipeline.yaml\n",
    "        learning_rate: [0.5, 1.0]\n",
    "        params_names: [[n_estimators, learning_rate]]\n",
    "        track: '{{track}}'\n",
    "        mlflow_tracking_uri: '{{mlflow_tracking_uri}}'\n",
    "    on_finish: hooks.store_report\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbd1b2",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing results in MLflow (UI)\n",
    "\n",
    "To review your experiments using MLflow's web application, click\n",
    "`File -> New -> Terminal` on the JupyterLab menu bar and execute the following\n",
    "command in the Terminal tab.\n",
    "\n",
    "```\n",
    "cd projects/mlflow/\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "*If you're running this example locally:*\n",
    "\n",
    "Upon initialization, go to [http://127.0.0.1:5000](http://127.0.0.1:5000) to open MLflow (`5000` is the default port, but it may change, check out the URL printed in the terminal)\n",
    "\n",
    "*If you're running this example on Binder:*\n",
    "\n",
    "The URL in your browser should look like this `https://{x}.mybinder.org/user/ploomber-{y}/lab/tree/projects/mlflow`. Copy the URL, open a new tab and delete everything after the `ploomber-y` portion (in our case, that would be `lab/tree/projects/mlflow`) and append `proxy/5000/` (note the trailing `/`), your URL should look like this: `https://{x}.mybinder.org/user/ploomber-{y}/proxy/5000/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066b19a",
   "metadata": {},
   "source": [
    "## Visualizing results in MLflow (CLI)\n",
    "\n",
    "If you're running this example locally, you may start MLflow with the following command:\n",
    "\n",
    "```sh\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "If you're using Binder, you can use MLflow's CLI to see the results. List experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mlflow experiments search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf67c5-1ea2-4674-84f0-2222278000ea",
   "metadata": {},
   "source": [
    "> **Note:** If you are using a Mlflow version prior to 2.0.1, please use the command `mlflow experiments list` to list all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add80ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mlflow runs list --experiment-id 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1157b-77a1-425f-b883-ec35a4a1292f",
   "metadata": {},
   "source": [
    "You can change the `--experiment-id` from 1 to another corresponding id to know more about its runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84e4ba",
   "metadata": {},
   "source": [
    "## Running experiments in parallel\n",
    "\n",
    "To run tasks in parallel, we must include the following in our `pipeline.yaml` file:\n",
    "\n",
    "```yaml\n",
    "executor: parallel\n",
    "\n",
    "# pipeline.yaml continues...\n",
    "```\n",
    "\n",
    "But if we want to run tasks serially, we can remove that line. For example, we may want to do this if each experiment needs too much memory and it's not possible to run them all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ceba84",
   "metadata": {},
   "source": [
    "## Selecting the best experiment\n",
    "\n",
    "The final task in our pipeline generates a notebook that prints the best experiment overall. We want this task to execute after all our experiments are done, but we cannot list all of them because the exact number may change (e.g., suppose you add more parameters to the random forest grid). Ploomber allows us wildcards to solve this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3529aeb",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```python\n",
    "# Content of scripts/compare.py\n",
    "\n",
    "# %% tags=[\"parameters\"]\n",
    "upstream = ['fit-*']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eb09b",
   "metadata": {},
   "source": [
    "Our `scripts/compare.py` declares upstream dependencies with the wildcard `fit-*`, which translates to \"execute all tasks that match the `fit-*` pattern before `scripts/compare.py`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32438d",
   "metadata": {},
   "source": [
    "## Using functions instead of scripts/notebooks\n",
    "\n",
    "Ploomber gives users the flexibility to use functions, scripts, or notebooks as tasks. If you prefer to use functions, you can use the same code and put it in a function. The advantage of using a script or notebook is that you can log all the output, just like we showed earlier."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
