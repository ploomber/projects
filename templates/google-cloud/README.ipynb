{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a640e80b",
   "metadata": {
    "papermill": {
     "duration": 0.036376,
     "end_time": "2022-05-17T23:07:25.589382",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.553006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To run this locally, [install Ploomber](https://docs.ploomber.io/en/latest/get-started/quick-start.html) and execute: `ploomber examples -n templates/google-cloud`\n",
    "\n",
    "Found an issue? [Let us know.](https://github.com/ploomber/projects/issues/new?title=templates/google-cloud%20issue)\n",
    "\n",
    "Questions? [Ask us on Slack.](https://ploomber.io/community/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d18220",
   "metadata": {
    "papermill": {
     "duration": 0.02895,
     "end_time": "2022-05-17T23:07:25.650259",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.621309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# google-cloud\n",
    "\n",
    "Note: this tutorial requires Ploomber 0.19.2 or higher.\n",
    "\n",
    "This tutorial will show how you can use Google Cloud and Ploomber to develop a scalable and production-ready pipeline.\n",
    "\n",
    "We'll use Google BigQuery (data warehouse) and Cloud Storage to show how we can transform big datasets with ease using SQL, plot the results with Python, and store the results in the cloud. Thanks to BigQuery scalability (we'll use a dataset with 5.5M records!) and Ploomber's convenience, **the entire process from importing the data to the summary report is on the cloud takes less than 20 seconds!**\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Before we begin, I'll quickly go over two Google Cloud services we use for this project. [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery) is a serverless data warehouse that allows us to analyze data at scale. In simpler terms, we can store massive datasets and query using SQL without managing servers. On the other hand, [Google Cloud Storage](https://en.wikipedia.org/wiki/Google_Cloud_Storage) is a storage service; it is the equivalent service to Amazon S3.\n",
    "\n",
    "Since our analysis comprises SQL and Python, we use [Ploomber](https://github.com/ploomber/ploomber), an open-source framework to write maintainable pipelines. It abstracts all the details, so we focus on writing the SQL and Python scripts.\n",
    "\n",
    "Finally, the data. We'll be using a public dataset that contains statistics of people's names in the US over time. The dataset contains 5.5M records. Here's what it looks like:\n",
    "\n",
    "![data](images/data.png)\n",
    "\n",
    "Let's now take a look at the pipeline's architecture!\n",
    "\n",
    "# Architecture overview\n",
    "\n",
    "![architecture](images/architecture.png)\n",
    "\n",
    "The first step is the `create-table.sql` script; such script runs a `CREATE TABLE` statement to copy a public dataset. `create-view.sql` and `create-materialized-view.sql` use the existing table and generate a view and a materialized view (their purpose is to show how we can create other types of SQL relations, we don't use the outputs).\n",
    "\n",
    "The `dump-table.sql` queries the existing table, and it dumps the results into a local file. Then, the `plot.py` Python script uses the local data file, generates a plot, and uploads it in HTML format to Cloud Storage. The whole process may seem intimidating, but Ploomber makes this straightforward!\n",
    "\n",
    "Let's now configure the cloud services we'll use!\n",
    "\n",
    "# Setup\n",
    "\n",
    "We need to create a bucket in Cloud Storage and a dataset in BigQuery; the following sections explain how to do so.\n",
    "\n",
    "## Cloud Storage\n",
    "\n",
    "Go to the [Cloud Storage](https://console.cloud.google.com/storage) console (select a project or create a new one, if needed) and create a new bucket (you may use an existing one if you prefer so). In our case, we'll create a bucket \"ploomber-bucket\" under the project \"ploomber\":\n",
    "\n",
    "![create-bucket](images/storage-create-bucket.png)\n",
    "\n",
    "Then, enter a name (in our case \"ploomber-bucket\"), and click on \"CREATE\":\n",
    "\n",
    "![confirm](images/storage-confirm.png)\n",
    "\n",
    "Let's now configure BigQuery.\n",
    "\n",
    "## BigQuery\n",
    "\n",
    "Go to the [BigQuery](https://console.cloud.google.com/bigquery) console and create a dataset. To do so, click on the three stacked dots next to your project's name and then click on \"Create dataset\":\n",
    "\n",
    "![create](images/bigquery-create.png)\n",
    "\n",
    "Now, enter \"my_dataset\" as the Dataset ID and \"us\" in *Data location* (location\n",
    "is important since we'll be using a public dataset located in such region),\n",
    "then click on \"CREATE DATASET\":\n",
    "\n",
    "![create](images/bigquery-confirm.png)\n",
    "\n",
    "Google Cloud is ready now! Let's now configure our local environment.\n",
    "\n",
    "## Local setup\n",
    "\n",
    "First, let's authenticate so we can make API calls to Google Cloud. Ensure\n",
    "you authenticate with an account that has enough permissions in the project\n",
    "to use BigQuery and Cloud Storage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488e60f",
   "metadata": {
    "papermill": {
     "duration": 0.026818,
     "end_time": "2022-05-17T23:07:25.705445",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.678627",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```sh\n",
    "gcloud auth login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c82e5",
   "metadata": {
    "papermill": {
     "duration": 0.029252,
     "end_time": "2022-05-17T23:07:25.763617",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.734365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you have trouble, check [the docs.](https://cloud.google.com/sdk/gcloud/reference/auth)\n",
    "\n",
    "Now, let's install Ploomber to get the code example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c5c78",
   "metadata": {
    "papermill": {
     "duration": 0.02759,
     "end_time": "2022-05-17T23:07:25.818697",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.791107",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```sh\n",
    "# note: this example requires ploomber 0.19.2 or higher\n",
    "pip install ploomber --upgrade\n",
    "\n",
    "# download example\n",
    "ploomber examples -n templates/google-cloud -o gcloud\n",
    "\n",
    "# move to the example folder\n",
    "cd gcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a2b27",
   "metadata": {
    "papermill": {
     "duration": 0.029807,
     "end_time": "2022-05-17T23:07:25.878701",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.848894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's now review the structure of the project.\n",
    "\n",
    "# Project structure\n",
    "\n",
    "* `pipeline.yaml` Pipeline declaration\n",
    "* `clients.py` Functions to create BigQuery and Cloud Storage clients\n",
    "* `requirements.txt` Python dependencies\n",
    "* `sql/` SQL scripts (executed in BigQuery)\n",
    "* `scripts/` Python scripts (executed locally, outputs uploaded to Cloud Storage)\n",
    "\n",
    "You can look at the files in detail [here.](https://github.com/ploomber/projects/tree/master/templates/google-cloud) For this tutorial, I'll quickly mention a few crucial details.\n",
    "\n",
    "`pipeline.yaml` is the central file in this project; Ploomber uses this file\n",
    "to assemble your pipeline and run it, here's what it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b1e7c",
   "metadata": {
    "papermill": {
     "duration": 0.026775,
     "end_time": "2022-05-17T23:07:25.932260",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.905485",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```yaml\n",
    "# Content of pipeline.yaml\n",
    "tasks:\n",
    "  # NOTE: ensure all products match the dataset name you created\n",
    "  - source: sql/create-table.sql\n",
    "    product: [my_dataset, my_table, table]\n",
    "\n",
    "  - source: sql/create-view.sql\n",
    "    product: [my_dataset, my_view, view]\n",
    "\n",
    "  - source: sql/create-materialized-view.sql\n",
    "    product: [my_dataset, my_materialized_view, view]\n",
    "\n",
    "  # dump data locally (and upload outputs to Cloud Storage)\n",
    "  - source: sql/dump-table.sql\n",
    "    product: products/dump.parquet\n",
    "    chunksize: null\n",
    "\n",
    "  # process data with Python (and upload outputs to Cloud Storage)\n",
    "  - source: scripts/plot.py\n",
    "    product: products/plot.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcec499",
   "metadata": {
    "papermill": {
     "duration": 0.029963,
     "end_time": "2022-05-17T23:07:25.989429",
     "exception": false,
     "start_time": "2022-05-17T23:07:25.959466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each task in the `pipeline.yaml` file contains two elements: the source code\n",
    "we want to execute and the product. You can see that we have a few SQL scripts\n",
    "that generate tables and views. However, the `dump-table.sql` creates a\n",
    "`.parquet` file. This indicates to Ploomber that it should download the results\n",
    "instead of storing them on BigQuery. Finally, the `plot.py` script contains an\n",
    "`.html` output; Ploomber will automatically run the script and store the\n",
    "results in the HTML file.\n",
    "\n",
    "You might be wondering how the order is determined. Ploomber extracts references\n",
    "from the source code itself; for example, the `create-view.sql` depends on\n",
    "`create-table.sql`. If we look at the code, we'll see the reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1cd92",
   "metadata": {
    "papermill": {
     "duration": 0.02794,
     "end_time": "2022-05-17T23:07:26.048460",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.020520",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```sql\n",
    "# Content of sql/create-view.sql\n",
    "DROP VIEW IF EXISTS {{ product }};\n",
    "CREATE VIEW {{ product }} AS\n",
    "SELECT *\n",
    "FROM {{ upstream[\"create-table\"] }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8763518",
   "metadata": {
    "papermill": {
     "duration": 0.032884,
     "end_time": "2022-05-17T23:07:26.109784",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.076900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is a placeholder `{{ upstream[\"create-table\"] }}`, this indicates\n",
    "that we should run `create-table.sql` first. At runtime, Ploomber will replace\n",
    "the placeholder for the table name. We also have a second placeholder\n",
    "`{{ product }}`, this will be replaced by the value in the `pipeline.yaml` file.\n",
    "\n",
    "That's it for the `pipeline.yaml`. Let's review the `clients.py` file.\n",
    "\n",
    "# Configure `clients.py`\n",
    "\n",
    "`clients.py` contains a function that returns clients to communicate with\n",
    "BigQuery and Cloud Storage.\n",
    "\n",
    "For example, this is how we connect to BigQuery:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0fbc55",
   "metadata": {
    "papermill": {
     "duration": 0.029957,
     "end_time": "2022-05-17T23:07:26.170559",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.140602",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```python\n",
    "# Content of clients.py\n",
    "def db():\n",
    "    \"\"\"Client to send queries to BigQuery\n",
    "    \"\"\"\n",
    "    return DBAPIClient(connect, dict())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d79e3d",
   "metadata": {
    "papermill": {
     "duration": 0.028921,
     "end_time": "2022-05-17T23:07:26.228731",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.199810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that we're returning a `ploomber.clients.DBAPIClient` object. Ploomber\n",
    "wraps BigQuery's connector, so it works with other databases.\n",
    "\n",
    "Secondly, we configure the Cloud Storage client:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2a344",
   "metadata": {
    "papermill": {
     "duration": 0.028077,
     "end_time": "2022-05-17T23:07:26.286036",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.257959",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```python\n",
    "# Content of clients.py\n",
    "def storage():\n",
    "    \"\"\"Client to upload files to Google Cloud Storage\n",
    "    \"\"\"\n",
    "    # ensure your bucket_name matches\n",
    "    return GCloudStorageClient(bucket_name='ploomber-bucket',\n",
    "                               parent='my-pipeline')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487e188",
   "metadata": {
    "papermill": {
     "duration": 0.027372,
     "end_time": "2022-05-17T23:07:26.341086",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.313714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we return a `ploomber.clients.GCloudStorageClient` object (ensure\n",
    "that the `bucket_name` matches yours!)\n",
    "\n",
    "Great, we're ready to run the pipeline!\n",
    "\n",
    "# Running the pipeline\n",
    "\n",
    "Ensure your terminal is open in the `gcloud ` folder and execute the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedf84e",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 0.026883,
     "end_time": "2022-05-17T23:07:26.396797",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.369914",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```sh\n",
    "# install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# run the pipeline\n",
    "ploomber build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144150c9",
   "metadata": {
    "papermill": {
     "duration": 0.028282,
     "end_time": "2022-05-17T23:07:26.453691",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.425409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After a few seconds of running the `ploomber build` command, you should see\n",
    "something like this:\n",
    "\n",
    "```txt\n",
    "name                      Ran?      Elapsed (s)    Percentage\n",
    "------------------------  ------  -------------  ------------\n",
    "create-table              True          5.67999      30.1718\n",
    "create-view               True          1.84277       9.78868\n",
    "create-materialized-view  True          1.566         8.31852\n",
    "dump-table                True          5.57417      29.6097\n",
    "plot                      True          4.16257      22.1113\n",
    "```\n",
    "\n",
    "If you get an error, you most likely have a misconfiguration. Please send us a\n",
    "[message on Slack](https://ploomber.io/community) so we can help you fix it!\n",
    "\n",
    "If you open the [BigQuery](https://console.cloud.google.com/bigquery) console, you'll see the new tables and views:\n",
    "\n",
    "![bigquery](images/bigquery.png)\n",
    "\n",
    "In the [Cloud Storage](https://console.cloud.google.com/storage) console, you'll see the HTML report:\n",
    "\n",
    "![storage](images/storage.png)\n",
    "\n",
    "Finally, if you download and open the HTML file, you'll see the plot!\n",
    "\n",
    "![plot](images/plot.png)\n",
    "\n",
    "# Incremental builds\n",
    "\n",
    "It may take a few iterations to get the final analysis. This process involves\n",
    "making small changes to your code and rerunning the workflow. Ploomber can\n",
    "keep track of source code changes to accelerate iterations, so it only\n",
    "executes outdated scripts next time. Enabling this requires a bit of extra\n",
    "configuration since Ploomber needs to store your pipeline's metadata, we\n",
    "already pre-configured the same workflow, so it stores the metadata in a SQLite\n",
    "database, you can run it with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d9717",
   "metadata": {
    "papermill": {
     "duration": 0.026612,
     "end_time": "2022-05-17T23:07:26.507996",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.481384",
     "status": "completed"
    },
    "region_name": "md",
    "tags": []
   },
   "source": [
    "```sh\n",
    "ploomber build --entry-point pipeline.incremental.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4557a",
   "metadata": {
    "papermill": {
     "duration": 0.027826,
     "end_time": "2022-05-17T23:07:26.564017",
     "exception": false,
     "start_time": "2022-05-17T23:07:26.536191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you run the command another time, you'll see that it skips all tasks:\n",
    "\n",
    "```txt\n",
    "name                      Ran?      Elapsed (s)    Percentage\n",
    "------------------------  ------  -------------  ------------\n",
    "create-table              False               0             0\n",
    "create-view               False               0             0\n",
    "create-materialized-view  False               0             0\n",
    "dump-table                False               0             0\n",
    "plot                      False               0             0\n",
    "```\n",
    "\n",
    "Now try changing `plot.py` and rerun the pipeline; you'll see that it skips\n",
    "most tasks!\n",
    "\n",
    "# Closing remarks\n",
    "\n",
    "This tutorial showed how to build maintainable and scalable data analysis pipelines on Google Cloud. Ploomber has many other features to simplify your\n",
    "workflow such as parametrization (store outputs on a different each time you\n",
    "run the pipeline), task parallelization, and even cloud execution (in case\n",
    "you need more power to run your Python scripts!).\n",
    "\n",
    "Check out our [documentation](https://docs.ploomber.io/) to learn more, and don't hesitate to [send us any questions!](https://ploomber.io/community)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.907685,
   "end_time": "2022-05-17T23:07:26.804678",
   "environment_variables": {},
   "exception": null,
   "input_path": "templates/google-cloud/_build/readme_preprocessed.ipynb",
   "output_path": "templates/google-cloud/README.ipynb",
   "parameters": {},
   "start_time": "2022-05-17T23:07:23.896993",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
